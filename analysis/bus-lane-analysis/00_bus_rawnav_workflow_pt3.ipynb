{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find nearest stops and segment ends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T13:06:33.310987Z",
     "start_time": "2020-09-20T13:06:32.376020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Run Section 1 Import Libraries and Set Global Parameters...\n"
     ]
    }
   ],
   "source": [
    "# 0 Housekeeping. Clear variable space\n",
    "########################################################################################################################\n",
    "from IPython import get_ipython  # run magic commands\n",
    "ipython = get_ipython()\n",
    "ipython.magic(\"reset -f\")\n",
    "ipython = get_ipython()\n",
    "#https://stackoverflow.com/questions/36572282/ipython-autoreload-magic-function-not-found\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")\n",
    "# 1 Import Libraries and Set Global Parameters\n",
    "########################################################################################################################\n",
    "# 1.1 Import Python Libraries\n",
    "############################################\n",
    "from datetime import datetime\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import shutil\n",
    "print(\"Run Section 1 Import Libraries and Set Global Parameters...\")\n",
    "begin_time = datetime.now()\n",
    "import os, sys, pandas as pd, geopandas as gpd\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")  # Stop Pandas warnings\n",
    "    \n",
    "path_working = r\"C:\\Users\\E048374\\OneDrive - WMATA\\rawnav_rachel_fork\\WMATA_AVL\"\n",
    "os.chdir(os.path.join(path_working))\n",
    "sys.path.append(r\"C:\\Users\\E048374\\OneDrive - WMATA\\rawnav_rachel_fork\\WMATA_AVL\")\n",
    "path_source_data = r\"\\\\l-600730\\RawNavArchive\"\n",
    "path_sp = r\"C:\\Users\\E048374\\Documents\\RawNav\"\n",
    "path_processed_data = os.path.join(path_working, \"data\", \"02-processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T13:06:33.479751Z",
     "start_time": "2020-09-20T13:06:33.313455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "q_jump_route_list = ['52']\n",
    "pattern_id = '5201' # see schedule filename below -- probably could just have full schedule..\n",
    "analysis_routes = q_jump_route_list\n",
    "analysis_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "wmata_crs = 2248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T13:06:34.048853Z",
     "start_time": "2020-09-20T13:06:33.482751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time Section 1 Import Libraries and Set Global Parameters : 0:00:00\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Import User-Defined Package\n",
    "############################################\n",
    "import wmatarawnav as wr\n",
    "\n",
    "executionTime = str(datetime.now() - begin_time).split('.')[0]\n",
    "print(\"Run Time Section 1 Import Libraries and Set Global Parameters : {}\".format(executionTime))\n",
    "print(\"*\" * 100)\n",
    "\n",
    "wmata_schedule_dat = (\n",
    "    pd.read_csv(\n",
    "        os.path.join(path_processed_data, f\"bus_sched_{pattern_id}.csv\")\n",
    "        ,dtype={'pattern':'int32','route':'str'}\n",
    "    )\n",
    ")\n",
    "\n",
    "wmata_schedule_gdf = (\n",
    "    gpd.GeoDataFrame(\n",
    "        wmata_schedule_dat, \n",
    "        geometry = gpd.points_from_xy(wmata_schedule_dat.stop_lon,wmata_schedule_dat.stop_lat),\n",
    "        crs='EPSG:4326'\n",
    "    )\n",
    "    .to_crs(epsg=wmata_crs)\n",
    ")\n",
    "\n",
    "# Make Output Directory\n",
    "path_stop_summary = os.path.join(path_processed_data, \"stop_summary.parquet\")\n",
    "if not os.path.isdir(path_stop_summary):\n",
    "    os.mkdir(path_stop_summary)\n",
    "\n",
    "path_stop_index = os.path.join(path_processed_data, \"stop_index.parquet\")\n",
    "if not os.path.isdir(path_stop_index):\n",
    "    os.mkdir(path_stop_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T13:11:00.483532Z",
     "start_time": "2020-09-20T13:09:28.629904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Processing analysis route 52\n",
      "Processing analysis route 52 for Monday...\n",
      "deleted 43 rows of 1000 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 3 of 957 stops with incorrect order from index table\n",
      "Processing analysis route 52 for Tuesday...\n",
      "deleted 32 rows of 880 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 0 of 848 stops with incorrect order from index table\n",
      "Processing analysis route 52 for Wednesday...\n",
      "deleted 13 rows of 440 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 1 of 427 stops with incorrect order from index table\n",
      "Processing analysis route 52 for Thursday...\n",
      "deleted 35 rows of 960 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 0 of 925 stops with incorrect order from index table\n",
      "Processing analysis route 52 for Friday...\n",
      "deleted 69 rows of 1400 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 16 of 1331 stops with incorrect order from index table\n",
      "Processing analysis route 52 for Saturday...\n",
      "deleted 30 rows of 480 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 0 of 450 stops with incorrect order from index table\n",
      "Processing analysis route 52 for Sunday...\n",
      "deleted 7 rows of 320 rows with distance to the nearest stop > 100 ft. from index table\n",
      "deleted 4 of 313 stops with incorrect order from index table\n",
      "Run Time Section Section 2: Read, analyze and summarize rawnav, WMATA schedule data : 0:04:27\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for analysis_route in analysis_routes:\n",
    "    print(\"*\" * 100)\n",
    "    print('Processing analysis route {}'.format(analysis_route))\n",
    "    for analysis_day in analysis_days:\n",
    "        print('Processing analysis route {} for {}...'.format(analysis_route,analysis_day))\n",
    "                \n",
    "        # Reload data\n",
    "        try:\n",
    "            rawnav_dat = (\n",
    "                wr.read_cleaned_rawnav(\n",
    "                   analysis_routes_ = analysis_route,\n",
    "                   analysis_days_ = analysis_day,\n",
    "                   path = os.path.join(path_processed_data, \"rawnav_data.parquet\")\n",
    "                )\n",
    "                .drop(columns=['blank', 'lat_raw', 'long_raw', 'sat_cnt'])\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)  # usually no data found or something similar\n",
    "            continue\n",
    "        else:\n",
    "\n",
    "            rawnav_summary_dat = (\n",
    "                wr.read_cleaned_rawnav(\n",
    "                    analysis_routes_ = analysis_route,\n",
    "                    analysis_days_ = analysis_day,\n",
    "                    path = os.path.join(path_processed_data, \"rawnav_summary.parquet\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Subset Rawnav Data to Records Desired\n",
    "            rawnav_summary_dat = rawnav_summary_dat.query('not (run_duration_from_sec < 600 | dist_odom_mi < 2)')\n",
    "            \n",
    "            rawnav_summary_keys_col = rawnav_summary_dat[['filename', 'index_run_start']]\n",
    "            \n",
    "            rawnav_qjump_dat = rawnav_dat.merge(rawnav_summary_keys_col,\n",
    "                                                on=['filename', 'index_run_start'],\n",
    "                                                how='right')\n",
    "\n",
    "            rawnav_qjump_gdf = (\n",
    "                gpd.GeoDataFrame(\n",
    "                    rawnav_qjump_dat,\n",
    "                    geometry=gpd.points_from_xy(rawnav_qjump_dat.long, rawnav_qjump_dat.lat),\n",
    "                    crs='EPSG:4326'\n",
    "                )\n",
    "                .to_crs(epsg=wmata_crs)\n",
    "            )\n",
    "\n",
    "        stop_summary, stop_index = (\n",
    "            wr.merge_rawnav_wmata_schedule(\n",
    "                analysis_route_=analysis_route,\n",
    "                analysis_day_=analysis_day,\n",
    "                rawnav_dat_=rawnav_qjump_gdf,\n",
    "                rawnav_sum_dat_=rawnav_summary_dat,\n",
    "                wmata_schedule_dat_=wmata_schedule_gdf\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if type(stop_summary) == type(None):\n",
    "            print('No data on analysis route {} for {}'.format(analysis_route,analysis_day))\n",
    "            continue\n",
    "        \n",
    "        # Write Summary Table \n",
    "        shutil.rmtree(\n",
    "            os.path.join(\n",
    "                path_stop_summary,\n",
    "                \"route={}\".format(analysis_route),\n",
    "                \"wday={}\".format(analysis_day)\n",
    "            ),\n",
    "            ignore_errors=True\n",
    "        ) \n",
    "        \n",
    "        pq.write_to_dataset(\n",
    "            table=pa.Table.from_pandas(stop_summary),\n",
    "            root_path=path_stop_summary,\n",
    "            partition_cols=['route', 'wday']\n",
    "        )\n",
    "        \n",
    "        # Write Index Table\n",
    "        shutil.rmtree(\n",
    "            os.path.join(\n",
    "                path_stop_index,\n",
    "                \"route={}\".format(analysis_route),\n",
    "                \"wday={}\".format(analysis_day)\n",
    "            ),\n",
    "            ignore_errors=True\n",
    "        ) \n",
    "        \n",
    "        stop_index = wr.drop_geometry(stop_index)\n",
    "        \n",
    "        stop_index = stop_index.assign(wday=analysis_day)\n",
    "                \n",
    "        pq.write_to_dataset(\n",
    "            table=pa.Table.from_pandas(stop_index),\n",
    "            root_path=path_stop_index,\n",
    "            partition_cols=['route', 'wday']\n",
    "        )\n",
    "\n",
    "executionTime = str(datetime.now() - begin_time).split('.')[0]\n",
    "print(\n",
    "      \"Run Time Section Section 2: Read, analyze and summarize rawnav, WMATA schedule data : {}\"\n",
    "      .format(executionTime)\n",
    ")\n",
    "print(\"*\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find nearest rawnav to segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T13:19:46.441495Z",
     "start_time": "2020-09-20T13:19:45.773411Z"
    }
   },
   "outputs": [],
   "source": [
    "segments = (\n",
    "    gpd.read_file(os.path.join(path_processed_data,\"seg_5201_by_intersection.geojson\"), dtype={'pattern':'int32'})\n",
    "    .to_crs(wmata_crs)\n",
    ")[['seg_name_id', 'name_str', 'geoid', 'stop_id',\n",
    "       'length', 'geometry']]\n",
    "\n",
    "seg_pattern = pd.read_csv(os.path.join(path_processed_data,\"stop_seq_pattern_5201_by_intersection.csv\"),\n",
    "                         dtype={'route':str, 'PATTERN_ID':str, 'pattern':'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T13:21:21.663519Z",
     "start_time": "2020-09-20T13:21:21.486615Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make Output Directory\n",
    "path_seg_summary = os.path.join(path_processed_data, \"segment_summary.parquet\")\n",
    "shutil.rmtree(path_seg_summary, ignore_errors=True) \n",
    "os.mkdir(path_seg_summary)\n",
    "\n",
    "path_seg_index = os.path.join(path_processed_data, \"segment_index.parquet\")\n",
    "shutil.rmtree(path_seg_index, ignore_errors=True) \n",
    "os.mkdir(path_seg_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-20T13:23:29.107122Z",
     "start_time": "2020-09-20T13:21:23.863711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Processing analysis route 52\n",
      "Processing analysis route 52 for Monday...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n",
      "Processing analysis route 52 for Tuesday...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n",
      "Processing analysis route 52 for Wednesday...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n",
      "Processing analysis route 52 for Thursday...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n",
      "Processing analysis route 52 for Friday...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n",
      "Processing analysis route 52 for Saturday...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n",
      "Processing analysis route 52 for Sunday...\n",
      "Processing segment 14th_22 ...\n",
      "Processing segment 14th_24 ...\n",
      "Processing segment 14th_25 ...\n"
     ]
    }
   ],
   "source": [
    "# 3 Merge Additional Geometry\n",
    "####################################################################################################\n",
    "\n",
    "# 3.1 Rawnav-Segment ########################\n",
    "# Iterate\n",
    "for analysis_route in analysis_routes:\n",
    "    print(\"*\" * 100)\n",
    "    print(f'Processing analysis route {analysis_route}')\n",
    "    for analysis_day in analysis_days:\n",
    "        print(f'Processing analysis route {analysis_route} for {analysis_day}...')\n",
    "        \n",
    "        # Reload data\n",
    "        try:\n",
    "            rawnav_dat = (\n",
    "                wr.read_cleaned_rawnav(\n",
    "                   analysis_routes_ = analysis_route,\n",
    "                   analysis_days_ = analysis_day,\n",
    "                   path = os.path.join(path_processed_data, \"rawnav_data.parquet\"))\n",
    "                .drop(columns=['blank', 'lat_raw', 'long_raw', 'sat_cnt'])\n",
    "                )\n",
    "        except:\n",
    "            print(f'No data on analysis route {analysis_route} for {analysis_day}')\n",
    "            continue\n",
    "        else:\n",
    "   \n",
    "            # Reload Data\n",
    "            rawnav_summary_dat = (\n",
    "                wr.read_cleaned_rawnav(\n",
    "                    analysis_routes_ = analysis_route,\n",
    "                    analysis_days_ = analysis_day,\n",
    "                    path = os.path.join(path_processed_data, \"rawnav_summary.parquet\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Subset Rawnav Data to Records Desired\n",
    "            rawnav_summary_dat = rawnav_summary_dat.query('not (run_duration_from_sec < 600 | dist_odom_mi < 2)')\n",
    "            \n",
    "            rawnav_qjump_dat = rawnav_dat.merge(rawnav_summary_dat[['filename', 'index_run_start']], \n",
    "                                                on=['filename', 'index_run_start'],\n",
    "                                                how='right')\n",
    "            \n",
    "            # Address Remaining Col Format issues\n",
    "            rawnav_qjump_gdf = (\n",
    "                gpd.GeoDataFrame(\n",
    "                    rawnav_qjump_dat, \n",
    "                    geometry = gpd.points_from_xy(\n",
    "                        rawnav_qjump_dat.long,\n",
    "                        rawnav_qjump_dat.lat\n",
    "                    ),\n",
    "                    crs='EPSG:4326')\n",
    "                .to_crs(epsg=wmata_crs)\n",
    "            )\n",
    "    \n",
    "            # Iterate on over Pattern-Segments Combinations Applicable to Route\n",
    "            xwalk_seg_pattern_subset = seg_pattern[['route','pattern','seg_name_id']].copy()\n",
    "                        \n",
    "            for seg in xwalk_seg_pattern_subset.seg_name_id.unique():\n",
    "                print('Processing segment {} ...'.format(seg))\n",
    "\n",
    "                # We pass the rawnav data and summary tables, check against a segment,\n",
    "                # and use the patterns_by_seg to indicate which patterns should be examined\n",
    "                index_run_segment_start_end, summary_run_segment = (\n",
    "                    wr.merge_rawnav_segment(\n",
    "                        rawnav_gdf_=rawnav_qjump_gdf,\n",
    "                        rawnav_sum_dat_=rawnav_summary_dat,\n",
    "                        target_=segments.loc[segments.seg_name_id == seg],\n",
    "                        patterns_by_seg_=xwalk_seg_pattern_subset.loc[xwalk_seg_pattern_subset.seg_name_id == seg]\n",
    "                    )\n",
    "                )\n",
    "                # Note that because seg_pattern_first_last is defined for route and pattern,\n",
    "                # our summary will implicitly drop any runs that are on 'wrong' pattern(s) for \n",
    "                # a route. \n",
    "                \n",
    "                index_run_segment_start_end['wday'] = analysis_day\n",
    "                summary_run_segment['wday'] = analysis_day\n",
    "                \n",
    "                # The additional partitioning here is excessive, but if fits better in the \n",
    "                # iterative/chunking process above\n",
    "                pq.write_to_dataset(\n",
    "                    table = pa.Table.from_pandas(summary_run_segment),\n",
    "                    root_path = path_seg_summary,\n",
    "                    partition_cols = ['route','wday','seg_name_id']\n",
    "                )\n",
    "                \n",
    "                pq.write_to_dataset(\n",
    "                    table = pa.Table.from_pandas(index_run_segment_start_end),\n",
    "                    root_path = path_seg_index,\n",
    "                    partition_cols = ['route','wday','seg_name_id']\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rawnav)",
   "language": "python",
   "name": "rawnav"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
